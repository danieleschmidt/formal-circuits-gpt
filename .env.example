# Formal-Circuits-GPT Environment Variables
# Copy this file to .env and fill in your values

# ====================
# LLM API Configuration
# ====================

# OpenAI API Key (for GPT-4, GPT-3.5-turbo)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API Key (for Claude models)
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional: OpenAI Organization ID
# OPENAI_ORG_ID=your_org_id_here

# ====================
# Application Configuration
# ====================

# Default LLM provider (openai, anthropic, local)
FCG_LLM_PROVIDER=openai

# Default model name
FCG_LLM_MODEL=gpt-4-turbo

# Default theorem prover (isabelle, coq)
FCG_DEFAULT_PROVER=isabelle

# Maximum refinement rounds for failed proofs
FCG_MAX_REFINEMENT_ROUNDS=5

# Parallel workers for batch processing
FCG_PARALLEL_WORKERS=4

# ====================
# Paths and Directories
# ====================

# Custom configuration directory
# FCG_CONFIG_DIR=~/.formal-circuits-gpt

# Cache directory for proofs and intermediate results
# FCG_CACHE_DIR=~/.formal-circuits-gpt/cache

# Log file location
# FCG_LOG_FILE=~/.formal-circuits-gpt/logs/fcg.log

# Custom Isabelle installation path
# ISABELLE_PATH=/usr/local/bin/isabelle

# Custom Coq installation path
# COQ_PATH=/usr/bin/coq

# ====================
# Logging and Debugging
# ====================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
FCG_LOG_LEVEL=INFO

# Enable debug mode (true/false)
FCG_DEBUG=false

# Enable verbose output (true/false)
FCG_VERBOSE=false

# Enable API request logging (true/false) 
# WARNING: This may log sensitive data like API keys
FCG_LOG_API_REQUESTS=false

# ====================
# Performance Tuning
# ====================

# LLM request timeout in seconds
FCG_LLM_TIMEOUT=120

# Theorem prover timeout in seconds
FCG_PROVER_TIMEOUT=300

# Maximum memory usage for provers (MB)
FCG_MAX_MEMORY=4096

# Enable proof caching (true/false)
FCG_ENABLE_CACHE=true

# Cache TTL in hours
FCG_CACHE_TTL=168

# ====================
# Security Settings
# ====================

# Enable input sanitization (true/false)
FCG_SANITIZE_INPUT=true

# Maximum file size for HDL inputs (MB)
FCG_MAX_FILE_SIZE=10

# Allowed file extensions (comma-separated)
FCG_ALLOWED_EXTENSIONS=.v,.vhd,.vhdl,.sv

# ====================
# Development Settings
# ====================

# Enable development mode (true/false)
FCG_DEV_MODE=false

# Enable hot reload in development (true/false)
FCG_HOT_RELOAD=false

# Test database URL (for testing)
# FCG_TEST_DB_URL=sqlite:///test.db

# ====================
# Cloud and CI/CD Settings
# ====================

# GitHub token for API access (if needed)
# GITHUB_TOKEN=your_github_token_here

# Docker registry (for container deployments)
# DOCKER_REGISTRY=ghcr.io/danieleschmidt

# Deployment environment (dev, staging, prod)
# FCG_ENVIRONMENT=dev

# ====================
# Monitoring and Analytics
# ====================

# Enable metrics collection (true/false)
FCG_ENABLE_METRICS=false

# Metrics endpoint URL
# FCG_METRICS_URL=http://localhost:9090

# Enable error reporting (true/false)
FCG_ENABLE_ERROR_REPORTING=false

# Error reporting service URL
# FCG_ERROR_REPORTING_URL=https://your-error-service.com

# ====================
# Example Values
# ====================
# Here are some example configurations for different use cases:

# Academic Research Setup:
# FCG_LLM_PROVIDER=openai
# FCG_LLM_MODEL=gpt-4-turbo
# FCG_DEFAULT_PROVER=isabelle
# FCG_MAX_REFINEMENT_ROUNDS=10
# FCG_LOG_LEVEL=DEBUG

# Production/Enterprise Setup:
# FCG_LLM_PROVIDER=anthropic
# FCG_LLM_MODEL=claude-3-5-sonnet
# FCG_DEFAULT_PROVER=coq
# FCG_PARALLEL_WORKERS=8
# FCG_ENABLE_METRICS=true
# FCG_LOG_LEVEL=INFO

# Local Development Setup:
# FCG_LLM_PROVIDER=openai
# FCG_LLM_MODEL=gpt-3.5-turbo
# FCG_DEFAULT_PROVER=isabelle
# FCG_DEV_MODE=true
# FCG_LOG_LEVEL=DEBUG
# FCG_VERBOSE=true

# ====================
# Security Notes
# ====================
# 
# 1. Never commit this file with real API keys to version control
# 2. Use different API keys for development, staging, and production
# 3. Rotate API keys regularly
# 4. Monitor API usage and set up billing alerts
# 5. Use environment-specific configuration files where possible
# 6. Consider using secrets management tools for production deployments